{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f41c8a59",
   "metadata": {},
   "source": [
    "# Building ETL Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5d0d7",
   "metadata": {},
   "source": [
    "### Installing necessary Python packages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89279ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade sodapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d562368",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee18db",
   "metadata": {},
   "source": [
    "### Part 1: Setting up 311 Calls NYC Open Data variables\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sodapy import Socrata\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "# suppressing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up host name for the API endpoint\n",
    "data_url = 'data.cityofnewyork.us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e185ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the 311 call data set at the API endpoint\n",
    "data_set = 'erm2-nwe9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91547f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up  App Token, which you created in Week 6\n",
    "app_token = 'IicNS1Wbw4TLdLPrHkKPeKAyb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the client that points to the API endpoint\n",
    "nyc_open_data_client = Socrata(data_url, app_token, timeout = 200)\n",
    "print(f\"nyc open data client name is: {nyc_open_data_client}\")\n",
    "print(f\"nyc open data client data type is: {type(nyc_open_data_client)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73884f99",
   "metadata": {},
   "source": [
    "### Setting up Google BigQuery variables\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abadea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS TO YOUR FILE PATH\n",
    "key_path = r'cis9440-340717-2ea1188a1979.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55dff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell without changing anything to setup your credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path,\n",
    "                                                                    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],)\n",
    "bigquery_client = bigquery.Client(credentials = credentials,\n",
    "                                 project = credentials.project_id)\n",
    "\n",
    "print(f\"bigquery client name is: {bigquery_client}\")\n",
    "print(f\"bigquery client data type is: {type(bigquery_client)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1185f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'cis9440-340717.final_project_etl'\n",
    "dataset_id = dataset_id.replace(':', '.')\n",
    "print(f\"your dataset_id is: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca4c60",
   "metadata": {},
   "source": [
    "### Extracting Data From 311 Calls Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4bb929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of records in the entire data set\n",
    "total_record_count = nyc_open_data_client.get(data_set, select = \"COUNT(*)\")\n",
    "print(f\"total records in {data_set}: {total_record_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of records in target data set\n",
    "target_record_count = nyc_open_data_client.get(data_set,\n",
    "                                               where = \"created_date >= '2021-01-01' and created_date <= '2022-04-30'\",\n",
    "                                               select = \"COUNT(*)\")\n",
    "print(f\"target records in {data_set}: {target_record_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ec236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data_in_chunks(target_record_count):\n",
    "    \n",
    "    # measure time this function takes\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    start = 0             # start at 0\n",
    "    chunk_size = 20000  # fetch 20000 rows at a time\n",
    "    results = []          # empty out our result list\n",
    "    record_count = target_record_count\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # fetch the set of records sta1rting at 'start'\n",
    "        results.extend(nyc_open_data_client.get(data_set,\n",
    "                                                where = \"created_date >= '2021-01-01' and created_date <= '2022-04-30'\",\n",
    "                                                offset = start,\n",
    "                                                limit = chunk_size))\n",
    "\n",
    "        # update the starting record number\n",
    "        start = start + chunk_size\n",
    "\n",
    "        # if we have fetched all of the records (we have reached record_count), exit loop\n",
    "        if (start > int(record_count[0]['COUNT'])):\n",
    "            break\n",
    "\n",
    "    # convert the list into a pandas data frame\n",
    "    data = pd.DataFrame.from_records(results)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"function took {round(end_time - start_time, 1)} seconds\")\n",
    "\n",
    "    print(f\"the shape of your dataframe is: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "data = pull_data_in_chunks(target_record_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed159226",
   "metadata": {},
   "source": [
    "### Profiling data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing columns in dataframe\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991b88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping location column\n",
    "data.drop([\"location\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ae9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and run a function to ceate data profiling dataframe\n",
    "\n",
    "def create_data_profiling_df(data):\n",
    "    \n",
    "    # create an empty dataframe to gather information about each column\n",
    "    data_profiling_df = pd.DataFrame(columns = [\"column_name\",\n",
    "                                                \"column_type\",\n",
    "                                                \"unique_values\",\n",
    "                                                \"duplicate_values\",\n",
    "                                                \"null_values\",\n",
    "                                                \"non_null_values\",\n",
    "                                                \"percent_null\"])\n",
    "\n",
    "    # loop through each column to add rows to the data_profiling_df dataframe\n",
    "    for column in data.columns:\n",
    "\n",
    "        info_dict = {}\n",
    "\n",
    "        try:\n",
    "            info_dict[\"column_name\"] = column\n",
    "            info_dict[\"column_type\"] = data[column].dtypes\n",
    "            info_dict[\"unique_values\"] = len(data[column].unique())\n",
    "            info_dict[\"duplicate_values\"] = (data[column].shape[0] - data[column].isna().sum()) - len(data[column].unique())\n",
    "            info_dict[\"null_values\"] = data[column].isna().sum()\n",
    "            info_dict[\"non_null_values\"] = data[column].shape[0] - data[column].isna().sum()\n",
    "            info_dict[\"percent_null\"] = round((data[column].isna().sum()) / (data[column].shape[0]), 3)\n",
    "\n",
    "        except:\n",
    "            print(f\"unable to read column: {column}, you may want to drop this column\")\n",
    "\n",
    "        data_profiling_df = data_profiling_df.append(info_dict, ignore_index=True)\n",
    "\n",
    "    data_profiling_df.sort_values(by = ['unique_values', \"non_null_values\"],\n",
    "                                  ascending = [False, False],\n",
    "                                  inplace=True)\n",
    "    \n",
    "    print(f\"data profiling complete, shape of df: {data_profiling_df.shape}\")\n",
    "    return data_profiling_df\n",
    "\n",
    "data_profiling_df = create_data_profiling_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view your data profiling dataframe\n",
    "data_profiling_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78e6be",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17367231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to look at a list of your columns\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTION REQUIRED\n",
    "# edit the drop_columns list below to include all the columns you would like to drop\n",
    "# then, run this cell to drop columns\n",
    "\n",
    "drop_columns = [\"unique_key\",\n",
    "               \"closed_date\",\n",
    "               \"street_name\",\n",
    "               \"location_type\", \n",
    "               \"incident_zip\",\n",
    "               \"incident_address\", \n",
    "               \"cross_street_1\",\n",
    "               \"cross_street_2\",\n",
    "               \"intersection_street_1\",\n",
    "               \"intersection_street_2\",\n",
    "               \"landmark\",\n",
    "               \"address_type\",\n",
    "               \"city\", \n",
    "               \"bbl\",\n",
    "               \"x_coordinate_state_plane\",\n",
    "               \"y_coordinate_state_plane\",\n",
    "               \"open_data_channel_type\",\n",
    "               \"park_facility_name\",\n",
    "               \"park_borough\",\n",
    "               \"latitude\",\n",
    "               \"longitude\", \n",
    "               \"facility_type\", \n",
    "               \"bridge_highway_name\",\n",
    "               \"bridge_highway_segment\",\n",
    "               \"bridge_highway_direction\",\n",
    "               \"taxi_company_borough\",\n",
    "               \"taxi_pick_up_location\",\n",
    "               \"road_ramp\",\n",
    "               \"vehicle_type\",\n",
    "               \"due_date\"]\n",
    "\n",
    "for column in drop_columns:\n",
    "    try:\n",
    "        data.drop(column, axis = 1, inplace = True)\n",
    "    except:\n",
    "        print(f\"unable to drop {column}\")\n",
    "\n",
    "print(f\"columns left in dataframe: {data.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of duplicate rows\n",
    "\n",
    "print(f\"number of duplicate rows: {len(data[data.duplicated()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd92da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows based on entire row\n",
    "data = data.drop_duplicates(keep = 'first')\n",
    "\n",
    "# Or, based on a subset of rows, uncomment below and adjust accordingly\n",
    "## data = data.drop_duplicates(subset = [\"subset column\"], keep = 'first')\n",
    "## data = data.drop_duplicates(subset = [\"subset column 1\", \"subset column 2\"], keep = 'first')\n",
    "\n",
    "print(f\"number of rows after duplicates dropped: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN rows based on entire row\n",
    "data = data.dropna()\n",
    "print(f\"number of rows after NaN dropped: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f8a03",
   "metadata": {},
   "source": [
    "### Creating Location Dimension (dim_location)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, copy the entire table\n",
    "dim_location = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6df1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_location.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, subset for only the wanted columns in the dimension\n",
    "dim_location = dim_location[[\"community_board\",\n",
    "                             \"borough\"]]\n",
    "\n",
    "#dim_location = dim_location[~dim_location['community_board'].isin(['Unspecified'])]\n",
    "dim_location = dim_location[dim_location[\"community_board\"].str.contains(\"Unspecified\")==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac401bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third, drop duplicate rows in dimension\n",
    "dim_location = dim_location.drop_duplicates(subset = [\"community_board\"], keep = 'first')\n",
    "dim_location = dim_location.reset_index(drop = True)\n",
    "\n",
    "dim_location = dim_location.sort_values(['borough', 'community_board'], ascending=[False, False])\n",
    "dim_location = dim_location.reindex(index=dim_location.index[::-1])\n",
    "\n",
    "#dim_location = dim_location.sort_values(\"borough\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth, add location_id as a surrogate key\n",
    "dim_location.insert(0, 'location_id', range(1000, 1000 + len(dim_location)))\n",
    "dim_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifth, add the location_id to the data table\n",
    "data = data.merge(dim_location[['community_board', 'location_id']],\n",
    "                  left_on = 'community_board',\n",
    "                  right_on = 'community_board',\n",
    "                  how = 'left')\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be369d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN rows based on entire row\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d47c4",
   "metadata": {},
   "source": [
    "### Creating Complaint Dimension (dim_complaint) \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3eda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, copy the entire table\n",
    "dim_complaint = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_complaint.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658412d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, subset for only the wanted columns in the dimension\n",
    "dim_complaint = dim_complaint[[\"descriptor\", \n",
    "                               \"complaint_type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third, drop duplicate rows in dimension\n",
    "dim_complaint = dim_complaint.drop_duplicates(subset = [\"descriptor\"], keep = 'first')\n",
    "dim_complaint = dim_complaint.reset_index(drop = True)\n",
    "dim_complaint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth, add complaint_id as a surrogate key\n",
    "dim_complaint.insert(0, 'complaint_id', range(10, 10 + len(dim_complaint)))\n",
    "dim_complaint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifth, add the complaint_id to the Fact table\n",
    "data = data.merge(dim_complaint[['descriptor', 'complaint_id']],\n",
    "                  left_on = 'descriptor',\n",
    "                  right_on = 'descriptor',\n",
    "                  how = 'left')\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aecafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN rows based on entire row\n",
    "data = data.dropna()\n",
    "# # converting new column to int object type\n",
    "# data[\"complaint_id\"] = data[\"complaint_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a712c",
   "metadata": {},
   "source": [
    "### Creating Date Dimension (dim_date) \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ac693",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ACTION REQUIRED: update the start and end date at the bottom of the sql_query variable to fit needs\n",
    "\n",
    "sql_query = \"\"\"\n",
    "            SELECT\n",
    "              CONCAT (FORMAT_DATE(\"%Y\",d),FORMAT_DATE(\"%m\",d),FORMAT_DATE(\"%d\",d)) as date_id,\n",
    "              d AS full_date,\n",
    "              FORMAT_DATE('%w', d) AS week_day,\n",
    "              FORMAT_DATE('%A', d) AS day_name,\n",
    "              FORMAT_DATE('%B', d) as month_name,\n",
    "              FORMAT_DATE('%Q', d) as fiscal_qtr,\n",
    "              FORMAT_DATE('%Y', d) AS year,\n",
    "            FROM (\n",
    "              SELECT\n",
    "                *\n",
    "              FROM\n",
    "                UNNEST(GENERATE_DATE_ARRAY('2021-01-01', '2022-04-30', INTERVAL 1 DAY)) AS d )\n",
    "            \"\"\"\n",
    "\n",
    "# store extracted data in new dataframe\n",
    "dim_date = bigquery_client.query(sql_query).to_dataframe()\n",
    "\n",
    "# validate that > 0 rows have been extracted and return dataframe\n",
    "if len(dim_date) > 0:\n",
    "    print(f\"date dimension created successfully, shape of dimension: {dim_date.shape}\")\n",
    "else:\n",
    "    print(\"date dimension FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e069d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create date_id column in the Fact Table\n",
    "data['date_id'] = data['created_date'].apply(lambda x: pd.to_datetime(x).strftime(\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN rows based on entire row\n",
    "data = data.dropna()\n",
    "# # converting new column to int object type\n",
    "# dim_date[\"date_id\"] = dim_date[\"date_id\"].astype(int)\n",
    "# data[\"date_id\"] = data[\"date_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b5d9e",
   "metadata": {},
   "source": [
    "### Creating Request Status Dimension (dim_request_status)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, copy the entire table\n",
    "dim_request_status = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05436a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_request_status.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, subset for only the wanted columns in the dimension\n",
    "dim_request_status = dim_request_status[[\"status\", \n",
    "                                         \"resolution_description\",\n",
    "                                         \"resolution_action_updated_date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa5c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third, drop duplicate rows in dimension\n",
    "dim_request_status = dim_request_status.drop_duplicates(subset = [\"status\"], keep = 'first')\n",
    "dim_request_status = dim_request_status.reset_index(drop = True)\n",
    "dim_request_status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24407752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth, add status_id as a surrogate key\n",
    "dim_request_status.insert(0, 'status_id', range(10, 10 + len(dim_request_status)))\n",
    "dim_request_status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c72fc74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fifth, add the status_id to the Fact table\n",
    "data = data.merge(dim_request_status[['status', 'status_id']],\n",
    "                  left_on = 'status',\n",
    "                  right_on = 'status',\n",
    "                  how = 'left')\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1268bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN rows based on entire row\n",
    "data = data.dropna()\n",
    "print(f\"number of rows after NaN dropped: {len(data)}\")\n",
    "# converting new column to int object type\n",
    "data[\"status_id\"] = data[\"status_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4f1a5",
   "metadata": {},
   "source": [
    "### Creating Agency Dimension (dim_agency)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, copy the entire table\n",
    "dim_agency = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_agency.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, subset for only the wanted columns in the dimension\n",
    "dim_agency = dim_agency[[\"agency\", \n",
    "                         \"agency_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third, drop duplicate rows in dimension\n",
    "dim_agency = dim_agency.drop_duplicates(subset = [\"agency\"], keep = 'first')\n",
    "dim_agency = dim_agency.reset_index(drop = True)\n",
    "dim_agency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth, add agency_id as a surrogate key\n",
    "dim_agency.insert(0, 'agency_id', range(10, 10 + len(dim_agency)))\n",
    "dim_agency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6297766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifth, add the agency_id to the Fact table\n",
    "data = data.merge(dim_agency[['agency', 'agency_id']],\n",
    "                  left_on = 'agency',\n",
    "                  right_on = 'agency',\n",
    "                  how = 'left')\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop NaN rows based on entire row\n",
    "# data = data.dropna()\n",
    "# print(f\"number of rows after NaN dropped: {len(data)}\")\n",
    "# # converting new column to int object type\n",
    "# data[\"agency_id\"] = data[\"agency_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85db1e3",
   "metadata": {},
   "source": [
    "### Creating fct_311_calls\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910893b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 311 Fact Table\n",
    "\n",
    "# creating a copy of the data table\n",
    "fct_311_calls = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad67b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of days_open\n",
    "# fct_311_calls[\"borough\"].hist(bins = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset of fact_table for only the needed columns: which are keys and measures\n",
    "fct_311_calls = fct_311_calls[[\"date_id\",\n",
    "                               \"complaint_id\",\n",
    "                               \"location_id\",\n",
    "                               \"status_id\",\n",
    "                               \"agency_id\"]]\n",
    "\n",
    "fct_311_calls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3dcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN rows based on entire row\n",
    "fct_311_calls = fct_311_calls.dropna()\n",
    "print(f\"number of rows after NaN dropped: {len(data)}\")\n",
    "# converting new column to int object type\n",
    "fct_311_calls[\"location_id\"] = fct_311_calls[\"location_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01499bb",
   "metadata": {},
   "source": [
    "### Delivering Fact and Dimensions to Data Warehouse (BigQuery)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc632314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to load dataframes to BigQuery\n",
    "\n",
    "def load_table_to_bigquery(df,\n",
    "                          table_name,\n",
    "                          dataset_id):\n",
    "\n",
    "    dataset_id = dataset_id\n",
    "\n",
    "    dataset_ref = bigquery_client.dataset(dataset_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.autodetect = True\n",
    "    job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "    upload_table_name = f\"{dataset_id}.{table_name}\"\n",
    "    \n",
    "    load_job = bigquery_client.load_table_from_dataframe(df,\n",
    "                                                upload_table_name,\n",
    "                                                job_config = job_config)\n",
    "        \n",
    "    print(f\"Starting job {load_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading location dimension to BigQuery\n",
    "\n",
    "# load_table_to_bigquery(df = dim_location,\n",
    "#                       table_name = \"dim_location\",\n",
    "#                       dataset_id = dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da6565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading date dimension to BigQuery\n",
    "\n",
    "load_table_to_bigquery(df = dim_date,\n",
    "                      table_name = \"dim_date\",\n",
    "                      dataset_id = dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading complaint dimension to BigQuery\n",
    "\n",
    "load_table_to_bigquery(df = dim_complaint,\n",
    "                      table_name = \"dim_complaint\",\n",
    "                      dataset_id = dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389199c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading request status dimension to BigQuery\n",
    "\n",
    "load_table_to_bigquery(df = dim_request_status,\n",
    "                      table_name = \"dim_request_status\",\n",
    "                      dataset_id = dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b982355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading agency dimension to BigQuery\n",
    "\n",
    "load_table_to_bigquery(df = dim_agency,\n",
    "                      table_name = \"dim_agency\",\n",
    "                      dataset_id = dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccbfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading 311 calls fact to BigQuery\n",
    "\n",
    "load_table_to_bigquery(df = fct_311_calls,\n",
    "                      table_name = \"fct_311_calls\",\n",
    "                      dataset_id = dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732663d9",
   "metadata": {},
   "source": [
    "### Part 2: Setting up Community District NYC Open Data variables\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the 311 call data set at the API endpoint\n",
    "data_set = 'jp9i-3b7y'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15523e19",
   "metadata": {},
   "source": [
    "### Extracting Data From Community District Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf12698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of records in the entire data set\n",
    "total_record_count = nyc_open_data_client.get(data_set, select = \"COUNT(*)\")\n",
    "print(f\"total records in {data_set}: {total_record_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617f880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the total number of records in target data set\n",
    "target_record_count = nyc_open_data_client.get(data_set,\n",
    "                                               select = \"COUNT(*)\")\n",
    "print(f\"target records in {data_set}: {target_record_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3cdd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data_in_chunks(target_record_count):\n",
    "    \n",
    "    # measure time this function takes\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    start = 0             # start at 0\n",
    "    chunk_size = 200000  # fetch 200000 rows at a time\n",
    "    results = []          # empty out our result list\n",
    "    record_count = target_record_count\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # fetch the set of records sta1rting at 'start'\n",
    "        results.extend(nyc_open_data_client.get(data_set,\n",
    "                                                offset = start,\n",
    "                                                limit = chunk_size))\n",
    "\n",
    "        # update the starting record number\n",
    "        start = start + chunk_size\n",
    "\n",
    "        # if we have fetched all of the records (we have reached record_count), exit loop\n",
    "        if (start > int(record_count[0]['COUNT'])):\n",
    "            break\n",
    "\n",
    "    # convert the list into a pandas data frame\n",
    "    data2 = pd.DataFrame.from_records(results)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"function took {round(end_time - start_time, 1)} seconds\")\n",
    "\n",
    "    print(f\"the shape of your dataframe is: {data2.shape}\")\n",
    "    return data2\n",
    "\n",
    "data2 = pull_data_in_chunks(target_record_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13110ad2",
   "metadata": {},
   "source": [
    "### Profiling Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da0c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing columns in dataframe\n",
    "data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dacd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and run a function to ceate data profiling dataframe\n",
    "\n",
    "def create_data_profiling_df(data2):\n",
    "    \n",
    "    # create an empty dataframe to gather information about each column\n",
    "    data_profiling_df = pd.DataFrame(columns = [\"column_name\",\n",
    "                                                \"column_type\",\n",
    "                                                \"unique_values\",\n",
    "                                                \"duplicate_values\",\n",
    "                                                \"null_values\",\n",
    "                                                \"non_null_values\",\n",
    "                                                \"percent_null\"])\n",
    "\n",
    "    # loop through each column to add rows to the data_profiling_df dataframe\n",
    "    for column in data2.columns:\n",
    "\n",
    "        info_dict = {}\n",
    "\n",
    "        try:\n",
    "            info_dict[\"column_name\"] = column\n",
    "            info_dict[\"column_type\"] = data2[column].dtypes\n",
    "            info_dict[\"unique_values\"] = len(data2[column].unique())\n",
    "            info_dict[\"duplicate_values\"] = (data2[column].shape[0] - data2[column].isna().sum()) - len(data2[column].unique())\n",
    "            info_dict[\"null_values\"] = data2[column].isna().sum()\n",
    "            info_dict[\"non_null_values\"] = data2[column].shape[0] - data2[column].isna().sum()\n",
    "            info_dict[\"percent_null\"] = round((data2[column].isna().sum()) / (data2[column].shape[0]), 3)\n",
    "\n",
    "        except:\n",
    "            print(f\"unable to read column: {column}, you may want to drop this column\")\n",
    "\n",
    "        data_profiling_df = data_profiling_df.append(info_dict, ignore_index=True)\n",
    "\n",
    "    data_profiling_df.sort_values(by = ['unique_values', \"non_null_values\"],\n",
    "                                  ascending = [False, False],\n",
    "                                  inplace=True)\n",
    "    \n",
    "    print(f\"data profiling complete, shape of df: {data_profiling_df.shape}\")\n",
    "    return data_profiling_df\n",
    "\n",
    "data_profiling_df = create_data_profiling_df(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d106820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view your data profiling dataframe\n",
    "data_profiling_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6bfe2",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to look at a list of your columns\n",
    "data2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTION REQUIRED\n",
    "# edit the drop_columns list below to include all the columns you would like to drop\n",
    "# then, run this cell to drop columns\n",
    "\n",
    "drop_columns = [\"shape_leng\",\n",
    "                \"shape_area\"]\n",
    "\n",
    "for column in drop_columns:\n",
    "    try:\n",
    "        data2.drop(column, axis = 1, inplace = True)\n",
    "    except:\n",
    "        print(f\"unable to drop {column}\")\n",
    "\n",
    "print(f\"columns left in dataframe: {data2.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b57398",
   "metadata": {},
   "source": [
    "### Creating Location Dimension (dim_location)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ab64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, copy the entire table\n",
    "dim_location = data2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2114c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_location.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_location[\"boro_cd\"] = dim_location[\"boro_cd\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c756d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign borough namde to community district number\n",
    "def borough_name(boro_cd):\n",
    "    if boro_cd >= 100 and boro_cd <= 199:\n",
    "        return \"Manhattan\"\n",
    "    elif boro_cd >= 200 and boro_cd <= 299:\n",
    "        return \"Bronx\"\n",
    "    elif boro_cd >= 300 and boro_cd <= 399:\n",
    "        return \"Brooklyn\"\n",
    "    elif boro_cd >= 400 and boro_cd <= 499:\n",
    "        return \"Queens\"\n",
    "    elif boro_cd >= 500 and boro_cd <= 599:\n",
    "        return \"Staten Island\"\n",
    "    \n",
    "# create a new column based on condition\n",
    "dim_location['borough'] = dim_location['boro_cd'].apply(borough_name)\n",
    "\n",
    "# display the dataframe\n",
    "dim_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dataframe by borough in alphabetical order\n",
    "dim_location = dim_location.sort_values('borough', ascending=False)\n",
    "dim_location = dim_location.reindex(index=dim_location.index[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddcbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add location_id as a surrogate key\n",
    "dim_location.insert(0, \"location_id\", range(1000, 1000 + len(dim_location)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[\"boro_cd\"] = data2[\"boro_cd\"].astype(int) \n",
    "dim_location[\"location_id\"] = dim_location[\"location_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcda736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding location_id to the data table\n",
    "data2 = data2.merge(dim_location[['boro_cd', 'location_id']],\n",
    "                  left_on = 'boro_cd',\n",
    "                  right_on = 'boro_cd',\n",
    "                  how = 'left')\n",
    "\n",
    "data2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a425c277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim_location = dim_location[[\"location_id\",\n",
    "                             \"boro_cd\",\n",
    "                             \"borough\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45048875",
   "metadata": {},
   "source": [
    "### Creating fct_community_district\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Community District Fact Table\n",
    "\n",
    "# creating a copy of the data table\n",
    "fct_community_district = data2[[\"location_id\",\n",
    "                               \"boro_cd\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fct_community_district.boro_cd = fct_community_district.boro_cd.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2f61f",
   "metadata": {},
   "source": [
    "### Delivering Fact and Dimensions to Data Warehouse (BigQuery)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to load dataframes to BigQuery\n",
    "\n",
    "def load_table_to_bigquery(df,\n",
    "                          table_name,\n",
    "                          dataset_id):\n",
    "\n",
    "    dataset_id = dataset_id\n",
    "\n",
    "    dataset_ref = bigquery_client.dataset(dataset_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.autodetect = True\n",
    "    job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "    upload_table_name = f\"{dataset_id}.{table_name}\"\n",
    "    \n",
    "    load_job = bigquery_client.load_table_from_dataframe(df,\n",
    "                                                upload_table_name,\n",
    "                                                job_config = job_config)\n",
    "        \n",
    "    print(f\"Starting job {load_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953cd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading location dimension to BigQuery\n",
    "\n",
    "load_table_to_bigquery(df = dim_location,\n",
    "                      table_name = \"dim_location\",\n",
    "                      dataset_id = dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a664e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading community district fact to BigQuery\n",
    "\n",
    "load_table_to_bigquery(df = fct_community_district,\n",
    "                      table_name = \"fct_community_district\",\n",
    "                      dataset_id = dataset_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
